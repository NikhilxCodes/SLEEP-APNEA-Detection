{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Q4d3EFSC7y3N",
        "outputId": "92127883-be5e-4d54-a10e-90e930b4a222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.11.15)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2025.3.2)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.15.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.20.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2025.6.15)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb) (1.17.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: EMD-signal in /usr/local/lib/python3.11/dist-packages (1.6.4)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.11/dist-packages (from EMD-signal) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.11/dist-packages (from EMD-signal) (1.15.3)\n",
            "Requirement already satisfied: pathos>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from EMD-signal) (0.3.4)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from EMD-signal) (4.67.1)\n",
            "Requirement already satisfied: ppft>=1.7.7 in /usr/local/lib/python3.11/dist-packages (from pathos>=0.2.1->EMD-signal) (1.7.7)\n",
            "Requirement already satisfied: dill>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pathos>=0.2.1->EMD-signal) (0.4.0)\n",
            "Requirement already satisfied: pox>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from pathos>=0.2.1->EMD-signal) (0.3.6)\n",
            "Requirement already satisfied: multiprocess>=0.70.18 in /usr/local/lib/python3.11/dist-packages (from pathos>=0.2.1->EMD-signal) (0.70.18)\n",
            "All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and install required packages\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages with correct names\n",
        "!pip install wfdb PyWavelets scikit-learn tensorflow keras numpy pandas matplotlib seaborn\n",
        "!pip install EMD-signal\n",
        "\n",
        "# Import all necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import wfdb\n",
        "import pywt  # This imports from PyWavelets package\n",
        "from PyEMD import EMD\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the apnea-ecg.zip file from Google Drive\n",
        "zip_path = '/content/drive/MyDrive/apnea-ecg.zip'\n",
        "extract_path = '/content/apnea_ecg_data/'\n",
        "\n",
        "# Create extraction directory\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset extracted successfully!\")\n",
        "\n",
        "# List the contents to verify extraction\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    print(f\"Directory: {root}\")\n",
        "    print(f\"Files: {files[:10]}\")  # Show first 10 files\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRF_vAJq8tFo",
        "outputId": "ef12f195-c1dc-4b43-cd0f-582e86cd8739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted successfully!\n",
            "Directory: /content/apnea_ecg_data/\n",
            "Files: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedECGPreprocessor:\n",
        "    def __init__(self, sampling_rate=100):\n",
        "        self.fs = sampling_rate\n",
        "        self.emd = EMD()\n",
        "\n",
        "    def extract_hjorth_features(self, signal):\n",
        "        \"\"\"Extract Hjorth parameters: Activity, Mobility, Complexity\"\"\"\n",
        "        try:\n",
        "            # Activity (variance)\n",
        "            activity = np.var(signal)\n",
        "            if activity == 0:\n",
        "                return [0, 0, 0]\n",
        "\n",
        "            # Mobility\n",
        "            diff1 = np.diff(signal)\n",
        "            mobility = np.sqrt(np.var(diff1) / activity)\n",
        "\n",
        "            # Complexity\n",
        "            diff2 = np.diff(diff1)\n",
        "            if len(diff2) == 0 or np.var(diff1) == 0 or mobility == 0:\n",
        "                return [activity, mobility, 0]\n",
        "\n",
        "            complexity = np.sqrt(np.var(diff2) / np.var(diff1)) / mobility\n",
        "\n",
        "            return [activity, mobility, complexity]\n",
        "        except:\n",
        "            return [0, 0, 0]\n",
        "\n",
        "    def wavelet_emd_decomposition(self, signal):\n",
        "        \"\"\"Perform Wavelet-EMD decomposition with error handling\"\"\"\n",
        "        try:\n",
        "            # Wavelet decomposition into 4 sub-bands\n",
        "            coeffs = pywt.wavedec(signal, 'db4', level=3)\n",
        "\n",
        "            features = []\n",
        "\n",
        "            # Process each wavelet sub-band\n",
        "            for i, coeff in enumerate(coeffs):\n",
        "                if len(coeff) > 10:  # Ensure sufficient length for EMD\n",
        "                    try:\n",
        "                        # EMD decomposition to get 5 IMFs\n",
        "                        imfs = self.emd(coeff, max_imf=5)\n",
        "\n",
        "                        # Extract Hjorth features from each IMF\n",
        "                        for imf in imfs:\n",
        "                            if len(imf) > 5:\n",
        "                                hjorth_features = self.extract_hjorth_features(imf)\n",
        "                                features.extend(hjorth_features)\n",
        "                    except:\n",
        "                        # Fallback: use original coefficient Hjorth features\n",
        "                        hjorth_features = self.extract_hjorth_features(coeff)\n",
        "                        features.extend(hjorth_features)\n",
        "\n",
        "            # Ensure consistent feature length\n",
        "            if len(features) == 0:\n",
        "                features = np.zeros(45)  # Default feature vector\n",
        "            else:\n",
        "                features = np.array(features)\n",
        "                # Pad or truncate to consistent length\n",
        "                if len(features) < 45:\n",
        "                    features = np.pad(features, (0, 45 - len(features)), 'constant')\n",
        "                elif len(features) > 45:\n",
        "                    features = features[:45]\n",
        "\n",
        "            return features\n",
        "        except:\n",
        "            return np.zeros(45)  # Return default features on error\n",
        "\n",
        "    def extract_rr_features(self, ecg_signal):\n",
        "        \"\"\"Extract traditional R-R interval features with fixed boolean indexing\"\"\"\n",
        "        try:\n",
        "            from scipy.signal import find_peaks\n",
        "\n",
        "            # Normalize signal\n",
        "            ecg_normalized = (ecg_signal - np.mean(ecg_signal)) / np.std(ecg_signal)\n",
        "\n",
        "            # Find R-peaks\n",
        "            peaks, _ = find_peaks(ecg_normalized, height=0.5, distance=50)\n",
        "\n",
        "            if len(peaks) < 2:\n",
        "                return np.zeros(10)  # Return zeros if insufficient peaks\n",
        "\n",
        "            # Calculate R-R intervals\n",
        "            rr_intervals = np.diff(peaks) / self.fs * 1000  # Convert to milliseconds\n",
        "\n",
        "            if len(rr_intervals) == 0:\n",
        "                return np.zeros(10)\n",
        "\n",
        "            # FIXED: Calculate pNN50 without boolean indexing error\n",
        "            rr_diff = np.diff(rr_intervals)\n",
        "            if len(rr_diff) == 0:\n",
        "                pnn50 = 0\n",
        "            else:\n",
        "                pnn50 = np.sum(np.abs(rr_diff) > 50)  # Fixed: removed boolean indexing\n",
        "\n",
        "            # Time domain features\n",
        "            features = [\n",
        "                np.mean(rr_intervals),           # Mean RR\n",
        "                np.std(rr_intervals),            # SDNN\n",
        "                np.sqrt(np.mean(rr_diff**2)) if len(rr_diff) > 0 else 0,  # RMSSD\n",
        "                pnn50,                           # pNN50 (fixed)\n",
        "                np.max(rr_intervals) - np.min(rr_intervals),  # Range\n",
        "                np.percentile(rr_intervals, 25), # Q1\n",
        "                np.percentile(rr_intervals, 75), # Q3\n",
        "                np.var(rr_intervals),            # Variance\n",
        "                len(peaks) / (len(ecg_signal) / self.fs),  # Heart rate\n",
        "                np.mean(np.abs(rr_diff)) if len(rr_diff) > 0 else 0     # Mean absolute deviation\n",
        "            ]\n",
        "\n",
        "            # Ensure no NaN or infinite values\n",
        "            features = [f if np.isfinite(f) else 0 for f in features]\n",
        "\n",
        "            return np.array(features)\n",
        "        except Exception as e:\n",
        "            print(f\"    RR feature extraction error: {e}\")\n",
        "            return np.zeros(10)\n",
        "\n",
        "# Initialize fixed preprocessor\n",
        "preprocessor = AdvancedECGPreprocessor()\n",
        "print(\"Fixed advanced preprocessor initialized!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w7GpkuA8vO1",
        "outputId": "7c3f60fb-3666-4910-85d6-a2a146a9079e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed advanced preprocessor initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_physionet_data_fixed(data_path):\n",
        "    \"\"\"Load PhysioNet Apnea-ECG data with proper segment-annotation alignment\"\"\"\n",
        "\n",
        "    # Find all .dat files (ECG recordings)\n",
        "    ecg_files = []\n",
        "    for root, dirs, files in os.walk(data_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.dat'):\n",
        "                ecg_files.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"Found {len(ecg_files)} ECG files\")\n",
        "\n",
        "    X_features = []\n",
        "    X_raw = []\n",
        "    y_labels = []\n",
        "\n",
        "    for file_path in ecg_files[:15]:  # Process first 15 files for demo\n",
        "        try:\n",
        "            # Extract record name\n",
        "            record_name = os.path.basename(file_path).replace('.dat', '')\n",
        "            record_path = file_path.replace('.dat', '')\n",
        "\n",
        "            print(f\"Processing {record_name}...\")\n",
        "\n",
        "            # Read ECG signal\n",
        "            record = wfdb.rdrecord(record_path)\n",
        "            ecg_signal = record.p_signal[:, 0]  # First channel\n",
        "\n",
        "            # Read annotations\n",
        "            try:\n",
        "                annotation = wfdb.rdann(record_path, 'apn')\n",
        "                apnea_labels = annotation.symbol\n",
        "            except:\n",
        "                print(f\"No annotations found for {record_name}, skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Calculate segments and ensure alignment\n",
        "            segment_length = 6000  # 1 minute at 100Hz\n",
        "            num_signal_segments = len(ecg_signal) // segment_length\n",
        "            num_annotation_labels = len(apnea_labels)\n",
        "\n",
        "            # Use the minimum to avoid index errors\n",
        "            num_segments_to_process = min(num_signal_segments, num_annotation_labels)\n",
        "\n",
        "            print(f\"  Signal segments: {num_signal_segments}, Annotation labels: {num_annotation_labels}\")\n",
        "            print(f\"  Processing {num_segments_to_process} segments\")\n",
        "\n",
        "            if num_segments_to_process == 0:\n",
        "                print(f\"  Skipping {record_name} - no valid segments\")\n",
        "                continue\n",
        "\n",
        "            segments_processed = 0\n",
        "            # Process segments\n",
        "            for i in range(num_segments_to_process):\n",
        "                start_idx = i * segment_length\n",
        "                end_idx = start_idx + segment_length\n",
        "\n",
        "                # Ensure we don't exceed signal length\n",
        "                if end_idx > len(ecg_signal):\n",
        "                    break\n",
        "\n",
        "                segment = ecg_signal[start_idx:end_idx]\n",
        "\n",
        "                # Skip if segment is too short\n",
        "                if len(segment) < segment_length:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Extract advanced features using the fixed preprocessor\n",
        "                    wavelet_emd_features = preprocessor.wavelet_emd_decomposition(segment)\n",
        "                    rr_features = preprocessor.extract_rr_features(segment)\n",
        "\n",
        "                    # Combine all features\n",
        "                    combined_features = np.concatenate([wavelet_emd_features, rr_features])\n",
        "\n",
        "                    # Skip if features are invalid\n",
        "                    if len(combined_features) == 0 or np.any(np.isnan(combined_features)) or np.any(np.isinf(combined_features)):\n",
        "                        print(f\"    Skipping segment {i} - invalid features\")\n",
        "                        continue\n",
        "\n",
        "                    X_features.append(combined_features)\n",
        "                    X_raw.append(segment)\n",
        "\n",
        "                    # Label encoding (A=apnea, N=normal)\n",
        "                    label = 1 if apnea_labels[i] == 'A' else 0\n",
        "                    y_labels.append(label)\n",
        "                    segments_processed += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error processing segment {i}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"  Successfully processed {segments_processed} segments from {record_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return np.array(X_features), np.array(X_raw), np.array(y_labels)\n",
        "\n",
        "# Load and process data with the fixed function\n",
        "print(\"Loading PhysioNet data with fixed alignment...\")\n",
        "X_features, X_raw, y = load_physionet_data_fixed(extract_path)\n",
        "\n",
        "print(f\"\\nFinal Results:\")\n",
        "print(f\"Loaded {len(X_features)} segments\")\n",
        "print(f\"Feature shape: {X_features.shape}\")\n",
        "print(f\"Raw signal shape: {X_raw.shape}\")\n",
        "print(f\"Labels shape: {y.shape}\")\n",
        "if len(y) > 0:\n",
        "    print(f\"Apnea ratio: {np.mean(y):.2%}\")\n",
        "    print(f\"Normal segments: {np.sum(y == 0)}\")\n",
        "    print(f\"Apnea segments: {np.sum(y == 1)}\")\n",
        "else:\n",
        "    print(\"No segments loaded - check data path and files\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gk-KCSg8xSG",
        "outputId": "014b0412-662f-4286-e8d5-d0e9e87f6ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading PhysioNet data with fixed alignment...\n",
            "Found 78 ECG files\n",
            "Processing a01r...\n",
            "  Signal segments: 492, Annotation labels: 489\n",
            "  Processing 489 segments\n",
            "    Skipping segment 7 - invalid features\n",
            "    Skipping segment 226 - invalid features\n",
            "    Skipping segment 351 - invalid features\n",
            "  Successfully processed 486 segments from a01r\n",
            "Processing c01...\n",
            "  Signal segments: 483, Annotation labels: 484\n",
            "  Processing 483 segments\n",
            "  Successfully processed 483 segments from c01\n",
            "Processing a04...\n",
            "  Signal segments: 496, Annotation labels: 492\n",
            "  Processing 492 segments\n",
            "  Successfully processed 492 segments from a04\n",
            "Processing x06...\n",
            "No annotations found for x06, skipping...\n",
            "Processing b01...\n",
            "  Signal segments: 486, Annotation labels: 487\n",
            "  Processing 486 segments\n",
            "  Successfully processed 486 segments from b01\n",
            "Processing b05...\n",
            "  Signal segments: 432, Annotation labels: 433\n",
            "  Processing 432 segments\n",
            "  Successfully processed 432 segments from b05\n",
            "Processing a01...\n",
            "  Signal segments: 492, Annotation labels: 489\n",
            "  Processing 489 segments\n",
            "  Successfully processed 489 segments from a01\n",
            "Processing x09...\n",
            "No annotations found for x09, skipping...\n",
            "Processing c02...\n",
            "  Signal segments: 501, Annotation labels: 502\n",
            "  Processing 501 segments\n",
            "  Successfully processed 501 segments from c02\n",
            "Processing a05...\n",
            "  Signal segments: 453, Annotation labels: 454\n",
            "  Processing 453 segments\n",
            "  Successfully processed 453 segments from a05\n",
            "Processing c10...\n",
            "  Signal segments: 430, Annotation labels: 431\n",
            "  Processing 430 segments\n",
            "  Successfully processed 430 segments from c10\n",
            "Processing a15...\n",
            "  Signal segments: 509, Annotation labels: 510\n",
            "  Processing 509 segments\n",
            "  Successfully processed 509 segments from a15\n",
            "Processing x31...\n",
            "No annotations found for x31, skipping...\n",
            "Processing a02...\n",
            "  Signal segments: 530, Annotation labels: 528\n",
            "  Processing 528 segments\n",
            "  Successfully processed 528 segments from a02\n",
            "Processing x07...\n",
            "No annotations found for x07, skipping...\n",
            "\n",
            "Final Results:\n",
            "Loaded 5289 segments\n",
            "Feature shape: (5289, 55)\n",
            "Raw signal shape: (5289, 6000)\n",
            "Labels shape: (5289,)\n",
            "Apnea ratio: 47.89%\n",
            "Normal segments: 2756\n",
            "Apnea segments: 2533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your loaded data to Google Drive\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "print(\"Saving dataset to Google Drive...\")\n",
        "\n",
        "try:\n",
        "    # Save arrays\n",
        "    np.save('/content/drive/MyDrive/X_raw_complete.npy', X_raw)\n",
        "    np.save('/content/drive/MyDrive/X_features_complete.npy', X_features)\n",
        "    np.save('/content/drive/MyDrive/y_labels_complete.npy', y)\n",
        "\n",
        "    # Save dataset info\n",
        "    dataset_info = {\n",
        "        'total_segments': len(y),\n",
        "        'feature_shape': X_features.shape,\n",
        "        'raw_shape': X_raw.shape,\n",
        "        'apnea_ratio': np.mean(y),\n",
        "        'normal_segments': np.sum(y == 0),\n",
        "        'apnea_segments': np.sum(y == 1)\n",
        "    }\n",
        "\n",
        "    with open('/content/drive/MyDrive/dataset_info.pkl', 'wb') as f:\n",
        "        pickle.dump(dataset_info, f)\n",
        "\n",
        "    print(\"✅ Dataset saved successfully!\")\n",
        "    print(f\"Files saved:\")\n",
        "    print(f\"  - X_raw_complete.npy: {X_raw.shape}\")\n",
        "    print(f\"  - X_features_complete.npy: {X_features.shape}\")\n",
        "    print(f\"  - y_labels_complete.npy: {y.shape}\")\n",
        "    print(f\"  - dataset_info.pkl: metadata\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error saving dataset: {e}\")\n",
        "\n",
        "# Verify the save worked\n",
        "try:\n",
        "    test_load = np.load('/content/drive/MyDrive/X_raw_complete.npy')\n",
        "    print(f\"✅ Verification: Successfully saved {len(test_load)} segments\")\n",
        "except:\n",
        "    print(\"❌ Verification failed - data may not be saved properly\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrCrDxX9jj-I",
        "outputId": "9180fc7b-d651-4e56-8ae1-77ff02e2f467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataset to Google Drive...\n",
            "✅ Dataset saved successfully!\n",
            "Files saved:\n",
            "  - X_raw_complete.npy: (5289, 6000)\n",
            "  - X_features_complete.npy: (5289, 55)\n",
            "  - y_labels_complete.npy: (5289,)\n",
            "  - dataset_info.pkl: metadata\n",
            "✅ Verification: Successfully saved 5289 segments\n"
          ]
        }
      ]
    }
  ]
}